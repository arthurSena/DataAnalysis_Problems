---
title: "Problema2 - Checkpoint3"
author: "Arthur Sena"
date: "03/14/2016"
output: html_document
---

#Descrição dos dados
Os dados são referentes aos usuários de ônibus de Curitiba, onde cada linha do arquivo segue o seguinte padrão: 

```{r echo=FALSE, warning=FALSE}
   library(plyr)
  library(rjson)
   json_file <- '[{"CODLINHA":"ARA","NOMELINHA":"SISTEMA ARAUCARIA","CODVEICULO":"00181","NUMEROCARTAO":"0002624774","DATAUTILIZACAO":"20/10/15 17:35:27,000000"},{"CODLINHA":"ARA","NOMELINHA":"SISTEMA ARAUCARIA","CODVEICULO":"00994","NUMEROCARTAO":"0002575080","DATAUTILIZACAO":"20/10/15 18:25:34,000000"},{"CODLINHA":"ARA","NOMELINHA":"SISTEMA ARAUCARIA","CODVEICULO":"00232","NUMEROCARTAO":"0002849283","DATAUTILIZACAO":"20/10/15 09:49:50,000000"},{"CODLINHA":"ARA","NOMELINHA":"SISTEMA ARAUCARIA","CODVEICULO":"00232","NUMEROCARTAO":"0001502669","DATAUTILIZACAO":"20/10/15 13:07:02,000000"},{"CODLINHA":"ARA","NOMELINHA":"SISTEMA ARAUCARIA","CODVEICULO":"00998","NUMEROCARTAO":"0003535943","DATAUTILIZACAO":"20/10/15 09:55:50,000000"},{"CODLINHA":"ARA","NOMELINHA":"SISTEMA ARAUCARIA","CODVEICULO":"00998","NUMEROCARTAO":"0003496859","DATAUTILIZACAO":"20/10/15 12:37:20,000000"}]'
   json_data <- fromJSON(json_file)
   data <- do.call("rbind", json_data)
   head(data)
```

#Usando SPARK
Nesse relatório, vamos responder duas perguntas com Spark. A primeira, que já foi respondida no checkpoint passado usando Hadoop, se encontra logo abaixo:

__a) Quais são os  pares (ou conjuntos de pessoas) que andam juntos ?__
Novamente, eu pretendo utilizar a mesma idéia, onde meu mapper retorna uma tupla, onde a chave é a concatenação do código do veículo com a data, hora e minuto. E o valor é o número do cartão. Desse modo, eu posso dizer que o passageiro com o cartão de número "XXXXXXX" pegou o veículo de código "XXXXX" na hora "XXXXXX". Lembrando que eu estou considerando até a precisão de minutos.

```{r, eval=FALSE}
      lines = sc.textFile("~/doc1-2015102121.txt") #Carrego o arquigo
    
      myMapper = lines.filter(lambda x : len(x) > 1)
                        .map(lambda x : x.strip().split(",")[4].split(":")[1] + ":" + x.strip().split(",")[4].split(":")[2]+""                              
                             +x.strip().split(',')[2].split(':')[1]+" " + x.strip().split(',')[3].split(":")[1])
    
    
      myReducer = myMapper.reduceByKey(lambda a, b: a+":"+b)
    
      resultado  = myReducer.map(lambda x: (x[0],x[1],len(x[1].split(":"))))
```

Por fim, eu escrevi o resultado em um arquivo, ordenei pelo quantidade de pessoas no grupo e abaixo é possível ver 'head' do resultado.

```{r, warning=FALSE}
      resultado_com_spark <- read.delim("~/Documents/DataAnalysis_Problems/DataCourse2/Problem2/resultado_com_spark.txt", header=FALSE, stringsAsFactors=FALSE,quote = "")
      colnames(resultado_com_spark) <- c("TEMPO_E_COD_VEIC" , "CARTOES" , "QTD_PESSOAS")
      temp <- head(resultado_com_spark[order(-resultado_com_spark[,3]),])
      rownames(temp) <- NULL
      temp
```

Podemos notar que o resultado é igual ao resultado obtido com o Hadoop e que o código usado é, consideravelmente, menor.


__b) Quais linhas apresentam mais veiculos ?__
Para resolver tal questão, eu vou fazer uma função map junto com a função "distinct" afim de retornar as tuplas não repetidas tendo como chave a linha e o código do veículo. Desse modo, eu posso aplicar um 'reducer' e contar quantos veículos cada linha contém.

```{r, eval=FALSE}
    myMapper = data.filter(lambda x : len(x) > 1).map(lambda x : (x.strip().split(",")[1].split(":")[1] , x.strip().split(",")[2].split(":")[1])).distinct()
  
  myReducer = myMapper.reduceByKey(lambda a, b: a+":"+b)

  resultado = myReducer.map(lambda x : (x[0],x[1],len(x[1].split(":"))))
 
```
 
O 'Head' do resultado contendo as linhas com mais veiculos pode ser visualizado logo abaixo:
 
```{r, warning=FALSE}
    resultado_com_spark2 <- read.delim("~/Documents/DataAnalysis_Problems/DataCourse2/Problem2/resultado_com_spark2.txt", header=FALSE, quote = "")
    colnames(resultado_com_spark2) <- c("LINHA" , "VEICULOS" , "QTD_VEIC")
      temp <- head(resultado_com_spark2[order(-resultado_com_spark2[,3]),])
      rownames(temp) <- NULL
      temp[,-2]
```

__c) Qual das duas tecnologias (i.e. Hadoop e Spark) apresentou melhor tempo de execução no quesito (a) acima.__

Para responder tal pergunta eu rodei novamente o código para responder a pergunta 5 vezes tanto para o Hadoop quanto para o Spark. No hadoop o tempo de execução de cada job pode ser visualizado logo abaixo:

* Launched At: 16-Mar-2016 01:17:42 (0sec)
Finished At: 16-Mar-2016 01:18:01 (19sec)

* Launched At: 15-Mar-2016 21:05:57 (0sec)
Finished At: 15-Mar-2016 21:06:17 (20sec)

* Launched At: 16-Mar-2016 02:19:00 (0sec)
Finished At: 16-Mar-2016 02:19:20 (19sec)

* Launched At: 16-Mar-2016 02:21:28 (0sec)
Finished At: 16-Mar-2016 02:21:48 (19sec)

* Launched At: 16-Mar-2016 02:22:46 (0sec)
Finished At: 16-Mar-2016 02:23:06 (19sec)

__Média: 19.2__

Os valores acima podem ser observado no arquivo "http://localhost:50030/jobhistoryhome.jsp" que contém um histórico com informações à respeito dos jobs rodados recentemente.

Quanto ao spark, eu usei a função time.time() para capturar o tempo de execução. Os valores foram os listados abaixo

* 3.76170611382 segundos
* 2.92090201378 segundos 
* 3.84272098541 segundos 
* 2.23719096184 segundos 
* 2.99762892723 segundos

__Média: 3.15203 seg__

Claramente vemos que o Spark apresentou um desempenho melhor em relação ao tempo de execução. 



